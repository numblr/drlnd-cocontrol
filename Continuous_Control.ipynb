{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "This repository is based on the continuous control problem studied in the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start the Environment\n",
    "\n",
    "The environment the agent operates on is based on a pre-built [Unity ML-Agent](https://github.com/Unity-Technologies/ml-agents) and is provided by the *CoControlEnv* class in the *cocontrol.environment* package.\n",
    "\n",
    "This class that is encapsulates the pre-built Unity environment, which must be downloaded to one of the following locations **_before running the code cells below_**:\n",
    "\n",
    "- **Mac**: `\"cocontrol/resources/Reacher.app\"`\n",
    "- **Windows** (x86): `\"cocontrol/resources/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"cocontrol/resources/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"cocontrol/resources/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"cocontrol/resources/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"cocontrol/resources/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"cocontrol/resources/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "It is important that the *cocontrol/resources* folder **_only contains the agent for your operating system_**!\n",
    "\n",
    "If the code cell below returns an error, please revisit the installation instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from unityagents.exception import UnityEnvironmentException\n",
    "\n",
    "PLATFORM_PATHS = ['Reacher.app',\n",
    "        'Reacher_Windows_x86/Reacher.exe',\n",
    "        'Reacher_Windows_x86_64/Reacher.exe',\n",
    "        'Reacher_Linux/Reacher.x86',\n",
    "        'Reacher_Linux_NoVis/Reacher.x86',\n",
    "        'Reacher_Linux/Reacher.x86_64',\n",
    "        'Reacher_Linux_NoVis/Reacher.x86_64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s CoControlEnv cocontrol/environment\n",
    "class CoControlEnv:\n",
    "    \"\"\"Banana collection environment.\n",
    "\n",
    "    The environment accepts actions and provides states and rewards in response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        for path in PLATFORM_PATHS:\n",
    "            try:\n",
    "                unity_resource = pkg_resources.resource_filename('cocontrol', 'resources/' + path)\n",
    "                self._env = UnityEnvironment(file_name=unity_resource)\n",
    "                print(\"Environment loaded from \" + path)\n",
    "                break\n",
    "            except UnityEnvironmentException as e:\n",
    "                print(\"Attempted to load \" + path + \":\")\n",
    "                print(e)\n",
    "                print(\"\")\n",
    "                pass\n",
    "\n",
    "        if not hasattr(self, '_env'):\n",
    "            raise Exception(\"No unity environment found, setup the environment as described in the README.\")\n",
    "\n",
    "        # get the default brain\n",
    "        self._brain_name = self._env.brain_names[0]\n",
    "        self._brain = self._env.brains[self._brain_name]\n",
    "\n",
    "        self._info = None\n",
    "        self._scores = None\n",
    "\n",
    "    def generate_episode(self, agent, max_steps=None, train_mode=False):\n",
    "        \"\"\"Create a generator for and episode driven by an actor.\n",
    "        Args:\n",
    "            actor: An actor that provides the next action for a given state.\n",
    "            max_steps: Maximum number of steps (int) to take in the episode. If\n",
    "                None, the episode is generated until a terminal state is reached.\n",
    "\n",
    "        Returns:\n",
    "            A generator providing a tuple of the current state, the action taken,\n",
    "            the obtained reward, the next state and a flag whether the next\n",
    "            state is terminal or not.\n",
    "        \"\"\"\n",
    "        states = self.reset(train_mode=train_mode)\n",
    "        is_terminal = False\n",
    "        count = 0\n",
    "\n",
    "        while not is_terminal and (max_steps is None or count < max_steps):\n",
    "            actions = agent.act(states)\n",
    "            rewards, next_states, is_terminals = self.step(actions)\n",
    "\n",
    "            step_data = (states, actions, rewards, next_states, is_terminals)\n",
    "\n",
    "            states = next_states\n",
    "            is_terminal = np.any(is_terminals)\n",
    "            count += 1\n",
    "\n",
    "            yield step_data\n",
    "\n",
    "    def reset(self, train_mode=False):\n",
    "        \"\"\"Reset and initiate a new episode in the environment.\n",
    "\n",
    "        Args:\n",
    "            train_mode: Indicate if the environment should be initiated in\n",
    "                training mode or not.\n",
    "\n",
    "        Returns:\n",
    "            The initial state of the episode (np.array).\n",
    "        \"\"\"\n",
    "        if self._info is not None and not np.any(self._info.local_done):\n",
    "            raise Exception(\"Env is active, call terminate first\")\n",
    "\n",
    "        self._info = self._env.reset(train_mode=train_mode)[self._brain_name]\n",
    "        self._scores = [0.0] * self.get_agent_size()\n",
    "\n",
    "        return self._info.vector_observations\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"Execute an action on all instances.\n",
    "\n",
    "        Args:\n",
    "            action: An tensor of ints representing the actions each instance.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the rewards (floats), the next states (np.array) and\n",
    "            a booleans indicating if the next state is terminal or not.\n",
    "        \"\"\"\n",
    "        if self._info is None:\n",
    "            raise Exception(\"Env is not active, call reset first\")\n",
    "\n",
    "        if torch.is_tensor(actions):\n",
    "            actions = actions.numpy()\n",
    "\n",
    "        self._info = self._env.step(actions)[self._brain_name]\n",
    "        next_states = self._info.vector_observations\n",
    "        rewards = self._info.rewards\n",
    "        is_terminals = self._info.local_done\n",
    "        self._scores += rewards\n",
    "\n",
    "        return rewards, next_states, is_terminals\n",
    "\n",
    "    def terminate(self):\n",
    "        self._info = None\n",
    "        self._score = None\n",
    "\n",
    "    def close(self):\n",
    "        self._env.close()\n",
    "        self._info = None\n",
    "\n",
    "    def get_score(self):\n",
    "        \"\"\"Return the cumulative reward of the current episode.\"\"\"\n",
    "        return self._score\n",
    "\n",
    "    def get_agent_size(self):\n",
    "        if self._info is None:\n",
    "            raise ValueError(\"No agents are initialized\")\n",
    "\n",
    "        return len(self._info.agents)\n",
    "\n",
    "    def get_action_size(self):\n",
    "        return self._brain.vector_action_space_size\n",
    "\n",
    "    def get_state_size(self):\n",
    "        return self._brain.vector_observation_space_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate episodes on the environment the *CoControlAgent* class provides and agent that selects actions in a particular state of the environement based on a policy function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s CoControlAgent cocontrol/environment\n",
    "class CoControlAgent:\n",
    "    \"\"\"Agent based on a policy approximator.\"\"\"\n",
    "\n",
    "    def __init__(self, pi):\n",
    "        \"\"\"Initialize the agent.\n",
    "\n",
    "        Args:\n",
    "            pi: policy-function that is callable with n states and returns a\n",
    "                (n, a)-dim array-like containing the value of each action.\n",
    "        \"\"\"\n",
    "        self._pi = pi\n",
    "\n",
    "    def act(self, states):\n",
    "        \"\"\"Select actions for the given states.\n",
    "\n",
    "        Args:\n",
    "            state: An array-like of states to choose the actions for.\n",
    "        Returns:\n",
    "            An array-like of floats representing the actions.\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(states):\n",
    "            try:\n",
    "                states = torch.from_numpy(states)\n",
    "            except:\n",
    "                states = torch.from_numpy(np.array(states, dtype=np.float))\n",
    "\n",
    "        states = states.float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return self._pi(states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the environment and run an episodewith a random or dummy policy. To run the test, uncomment the *test_env()* invocation and choose the policy you like.\n",
    "\n",
    "**_After you ran the test you need to comment it out again and restart the kernel, as recreating the environment does not work!_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env():\n",
    "    env = CoControlEnv()\n",
    "\n",
    "    # all actions are between -1 and 1\n",
    "    dummy_pi = lambda s: torch.rand(env.get_agent_size(), env.get_action_size()) * 2.0 - 1.0\n",
    "\n",
    "    # Alternative dummy policy:\n",
    "    #\n",
    "    # cnt = 0\n",
    "    # dummy_pi = lambda s: torch.ones(env.get_agent_size(), env.get_action_size()) \\\n",
    "    #     * ((cnt % 10)/10 * 2 - 1) * -1 ** (cnt % 10)\n",
    "\n",
    "    agent = CoControlAgent(dummy_pi)\n",
    "    episode = enumerate(env.generate_episode(agent, max_steps = 1000))\n",
    "    for count, step_data in episode:\n",
    "        # Consume the generated steps\n",
    "        cnt = count\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "# test_env()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
