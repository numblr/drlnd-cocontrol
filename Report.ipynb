{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "This repository is based on the continuous control problem studied in the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program and demonstrates a variant of the PPO learning algorithm in an environment with a continuous action space.\n",
    "\n",
    "![Actor](resources/small_agent.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General description\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.04 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.\n",
    "\n",
    "The environment is implemented as a Unity environment available in two versions:\n",
    "- A version that contains a single agent.\n",
    "- A version that contains 20 identical agents, each with its own copy of the environment.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, the average of the total score of an epsidoe over all agents over 100 consecutive episodes must be above +30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm\n",
    "\n",
    "The algorithm used is a variant of Proximal Policy Optimization (PPO) with a Generalized Advantage Estimation (GAE) and a clipped objective function. The algorithm uses a finite horizon instead of reward discounting. In the following we give a very brief summary of the elements, a more detailed overview can be found in the referenced sources or e.g. in this two lecture from the [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures): [Policy gradients and GAE](https://www.youtube.com/watch?v=S_gwYj1Q-44), [PPO](https://www.youtube.com/watch?v=xvRrgxcpaHY).\n",
    "\n",
    "### 2.1 Proximal Policy Optimization (PPO)\n",
    "\n",
    "Proximal Policy Optimization is a variant of policy gradient methods that is used to improve sample efficiency. Policy gradient methods are based on the objective to maximize the expectation $\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[R(\\tau)\\right]$ of the total reward $R(\\tau)$ for trajectories $\\tau=(s_0, a_0, r_0, s_1, a_1, r_1, \\dots)$ under a parameterized policy $\\pi_\\theta(\\tau)$.\n",
    "\n",
    "The gradient of the above objective can be derived as \n",
    "\n",
    "$$\n",
    "g\n",
    "= \\nabla_\\theta \\, \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\!\\! \\left[ \\sum_{t=0}^H \\log{\\pi_\\theta(a_t|s_t)} (R_t^f - b(s_t) \\right]\n",
    "= \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^H \\frac{\\nabla_\\theta \\pi_\\theta(a_t|s_t)}{\\pi_\\theta(a_t|s_t)} (R_t^f - b(s_t) \\right]\n",
    "$$\n",
    "\n",
    "where $R_f^t$ denotes the future reward at time step $t$ and $b(s_t)$ is a baseline which can depend on the state $s_t$. An equivalent formulation can be obtained by looking at the expectation with respect to a different policy $\\pi_{\\theta'}$:\n",
    "\n",
    "$$\n",
    "g\n",
    "= \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^H \\frac{\\nabla_\\theta \\pi_\\theta(a_t|s_t)}{\\pi_\\theta(a_t|s_t)} (R_t^f - b(s_t) \\right]\n",
    "= \\mathbb{E}_{\\tau \\sim \\pi_{\\theta'}}  \\left[ \\sum_{t=0}^H \\frac{\\left. \\nabla_\\theta \\pi_\\theta(a_t|s_t) \\right|_{\\theta = \\theta'}}{\\pi_{\\theta'}(a_t|s_t)} (R_t^f - b(s_t) \\right].\n",
    "$$\n",
    "\n",
    "When the above expectations are approximated by data from sampled trajectories, this implies that we can estimate the gradient for an evolved policy $\\pi_\\theta$ from trajectories that were sampled using the old policy $\\pi_{\\theta'}$ reasonably well if $\\theta$ is in a proximity of $\\theta'$. In other words, we can reuse trajectories generated with $\\pi_{\\theta'}$ to further optimize $\\pi_{\\theta}$, as long as $\\pi_{\\theta}$ does not get too far from $\\pi_{\\theta'}$.\n",
    "The above gradient can be obtained by diffentiating the objective function \n",
    "\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta'}}  \\left[ \\sum_{t=0}^H \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta'}(a_t|s_t)} (R_t^f - b(s_t) \\right].\n",
    "$$\n",
    "\n",
    "As proposed in this [article](https://arxiv.org/abs/1707.06347), an easy and efficient way protect against the inacurracy introduced by a too large deviation of $\\theta$ from $\\theta'$ is to clip the ratio $r(\\theta) = \\frac{\\pi_{\\theta}}{\\pi_{\\theta'}}$ to a maximum value of $1 + \\epsilon$\n",
    "\n",
    "$$\n",
    "L^{PPO}(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta'}} \\left [\n",
    "    \\min(r(\\theta)(R_t^f - b(s_t)), \\rm{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon) (R_t^f - b(s_t)))\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "where the clipping parameter $\\epsilon$ is a hyper parameter of the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generalized Advantage Estimation (GAE) Critic\n",
    "\n",
    "As a baseline $b(s_t)$ we use the state-value function $V_{\\pi_\\theta}(s_t)$, and replace the term $(R_t^f - V_{\\pi_\\theta}(s_t))$ with a function approximator $\\tilde A(s, a)$ (critic). $\\tilde A(s, a)$ is an approximation of the advantage function $A_{\\pi_\\theta}(s, a) = Q_{\\pi_\\theta}(s,a) - V_{\\pi_\\theta}(s)$. To obtain this estimate we use the generalized advantage estimator as described in [this article](https://arxiv.org/abs/1506.02438), which approximates the advantage function using the state-value function as a weighted average over n-step temporal differences $r_t + r_{t+1} + \\cdots + r_{t+n-1} + V(s_{t+n}) - V(s_{t})$:\n",
    "\n",
    "$$\n",
    "\\tilde A^{GAE} = - \\tilde V_{\\pi_\\theta}(s_t) + (1-\\lambda)\\sum_{k=t}^H \\lambda^{(k-t)} \\left(\\sum_{l=t}^{k-1} r(s_l) + \\tilde V(s_k)\\right) = \\sum_{k=t}^H \\lambda^{(k-t)}\\left(r_{k} + \\tilde V(s_{k+1}) - \\tilde V(s_{k})\\right) = \\sum_{k=t}^H \\lambda^{(k-t)}\\delta_k^{\\tilde V},\n",
    "$$\n",
    "where $\\delta_t^V = r_t + V(s_{t+1}) - V(s_{t})$.\n",
    "\n",
    "The estimate $\\tilde V$ of the state-value is obtained by minimizing the error between the estimated and sampled state-values for a paramterized estimator $\\tilde V_\\phi(s_{t})$:\n",
    "$$\n",
    "L^V = \\hat{\\mathbb{E}} \\left[ \\| R_t^f - \\tilde V_\\phi(s_{t}) \\| \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Finite horizon and noise reduction\n",
    "\n",
    "Instead of discounting of rewards we decided to consider a fixed size finite horizon for the agent. This is implemented by generating full episodes, splitting them with a rolling window corresponding to the horizon of the agent and removing windows in which the agent does not obtain any reward. \n",
    "\n",
    "![windows](resources/windows.png)\n",
    "\n",
    "The reason for this is the following: Though the task is setup to be episodic, it is continuous in it's nature, and actions should have a comparable short term effect. From this observation it seems reasonable to assume that a finite horizon should be sufficient for the agent to solve the task. Further, looking at the actor at the beginning of the learning process, it is evident that it suffers from find a good starting point, as the probability to reach the target by accident is low. Implemening a fixed horizon instead of discounting gives us an easy way to remove windows in which the agent doesn't retrieve any reward. Considering that we can understand the learning process as a directed search driven by the obtained reward, it seems reasonable to assume that these windows do not contribute in a positive way to the learning process.\n",
    "\n",
    "### 2.3 Approximators and training\n",
    "\n",
    "For both the approximator $\\pi_\\theta$ as well as the state-value function $V_\\phi$ we use fully connected neural networds with two hidden layers of size 128 and 64 with relu activations. The two networks differ only in the output layers, as described in the following. The approximators are trained in batches and a replay buffer is used, i.e. the  order of timestpes is randomized during training.\n",
    "\n",
    "#### 2.3.1 Policy approximator\n",
    "\n",
    "The policy approximator maps a state to a normal distribution  with a fixed variance and an estimated mean for each of the available actions.  The normal distributions are truncated to the interval $[-1,1]$ to conform to the allowed values for the actions. The mean estimate is provided by a neural network as described above, where the size of the output layer corresponds to the number of available actions and uses a $\\tanh$ activation function to confine the output to the interval $[-1,1]$.\n",
    "\n",
    "![Actor net](resources/actor_net.png)\n",
    "\n",
    "#### 2.3.2 State-Value approximator\n",
    "\n",
    "The approximator for the state value function maps a state to a real number and is implemented by a neural network as described above with a single node in the output layer without additional activation.\n",
    "\n",
    "![Critic net](resources/critic_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Algorithm:\n",
    "\n",
    "* Init $\\theta'$, $\\theta$, $\\phi$\n",
    "* For $N$ epochs:\n",
    "    * Set $\\theta' = \\theta$\n",
    "    * Generate $M$ episodes from each agent with respect to $\\pi_{\\theta'}$\n",
    "    * Split the episodes into windows of length $m$ using a rolling window\n",
    "    * Remove windows where no reward is achived\n",
    "    * Calculate $\\pi_{\\theta'}(a_t|s_t)$ and $A^{GAE}(s_t, a_t)$ for all time steps in each window\n",
    "    * For $k$ epochs:\n",
    "        * Update $\\theta$ to optimize $L^{PPO}$ using sampled a batches of $j$ windows\n",
    "        * Update $\\phi$ to optimize $L^V$ using the same batches as in the previous step\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Further remarks on the algorithm and chosen hyper parameters\n",
    "\n",
    "* For simplicity we limited the length of the generated episodes to 1000 steps during training which allows us to ignore terminal states.\n",
    "* The horizon of the agent is limited to 50 steps and from each episode sliding windows with a offset of 5 steps are extracted. This resulting in potentially $(20 + 24 \\cdot 19)\\cdot 20 = 9520$ windows of 50 steps per episode from 20 agents, from which windows where no reward is obtained are filtered out.\n",
    "* No reward discounting is used as we already limit the horizon of the agent. \n",
    "* Though clipping of the surrogate objective is essential, the clip window can be keept quite large at 0.75.\n",
    "* The parameter for the GAE is set to 0.75 to provide a noticable amount of variance reduction. \n",
    "* After generating 8 epidsodes from the 20 agents, learning is executed on 8 epochs on the sampled data in batches of size 32 and a learning rate of $10^{-4}$.   \n",
    "* Gradient clipping turned out to be not necessary to solve the environment.\n",
    "* The variance for the normal distributions is set to 0.5. This is a rather high value considering the range for the action values is the interval $[-1,1]$, but on the one hand a too low variance will cause new policy estimates to differ very quickly from old ones, and on the other hand the variance seems to cancel out over consecutive timesteps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Results and performance\n",
    "The following plot shows the total scores per generated episode, averaged over the 20 agents in each generatino step.\n",
    "As learning takes only takes place each 8 episodes, improvement happens gradually at each 8th episode. The agent reaches an average of +30 over 100 consecutive episodes during the learning process after 128 generated episodes (for 20 agents). The target score itself is, however, reached already after 56 episodes.\n",
    "\n",
    "![Simple DNN](scores.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Improvements\n",
    "\n",
    "The agent already seems to come close to the optimal policy, but points were performance could still improved are the inital phase of the episode, i.e. the time the agent needs to first reach the target, as well as reliability, i.e. preventing that the agent looses the target again.\n",
    "\n",
    "To reach the first point a possibility could be experiment with (progressively) shorter horizons for the agent, as that could force it to achive reward within fewer timesteps. \n",
    "\n",
    "For the second point it would be interesting to investigate if it is possible to reduce the variance of the policy approximator during training. This has to go along with a more aggressive clipping of the objective function and eventually a decay in the learning rate. If this can be successful, however, will also depend on the noise present in the sampled trajectories, as that will unavoidable lead to fluctuations of the policy approximator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation\n",
    "\n",
    "In this section we provide a detailed walk through the implementation. To reproduce the above results, however, run the cocontrol.py script from the command line as described in the README without command line parameters.\n",
    "\n",
    "First we run some neccessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pkg_resources\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions.normal as dist\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from unityagents.exception import UnityEnvironmentException\n",
    "\n",
    "from cocontrol.util import plot\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "PLATFORM_PATHS = ['Reacher.app',\n",
    "        'Reacher_Windows_x86/Reacher.exe',\n",
    "        'Reacher_Windows_x86_64/Reacher.exe',\n",
    "        'Reacher_Linux/Reacher.x86',\n",
    "        'Reacher_Linux_NoVis/Reacher.x86',\n",
    "        'Reacher_Linux/Reacher.x86_64',\n",
    "        'Reacher_Linux_NoVis/Reacher.x86_64']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Start the Environment\n",
    "\n",
    "The environment the agent operates on is based on a pre-built [Unity ML-Agent](https://github.com/Unity-Technologies/ml-agents) and is provided by the *CoControlEnv* class in the *cocontrol.environment* package.\n",
    "\n",
    "This class encapsulates the pre-built Unity environment, which must be downloaded to one of the following locations **_before running the code cells below_**:\n",
    "\n",
    "- **Mac**: `\"cocontrol/resources/Reacher.app\"`\n",
    "- **Windows** (x86): `\"cocontrol/resources/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"cocontrol/resources/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"cocontrol/resources/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"cocontrol/resources/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"cocontrol/resources/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"cocontrol/resources/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "It is important that the *cocontrol/resources* folder **_only contains the agent for your operating system_**!\n",
    "\n",
    "If the code cell below returns an error, please revisit the installation instructions in the README and double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s CoControlEnv cocontrol/environment\n",
    "class CoControlEnv:\n",
    "    \"\"\"Continuous control environment.\n",
    "\n",
    "    The environment accepts actions and provides states and rewards in response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        for path in PLATFORM_PATHS:\n",
    "            try:\n",
    "                unity_resource = pkg_resources.resource_filename('cocontrol', 'resources/' + path)\n",
    "                self._env = UnityEnvironment(file_name=unity_resource)\n",
    "                print(\"Environment loaded from \" + path)\n",
    "                break\n",
    "            except UnityEnvironmentException as e:\n",
    "                print(\"Attempted to load \" + path + \":\")\n",
    "                print(e)\n",
    "                print(\"\")\n",
    "                pass\n",
    "\n",
    "        if not hasattr(self, '_env'):\n",
    "            raise Exception(\"No unity environment found, setup the environment as described in the README.\")\n",
    "\n",
    "        # get the default brain\n",
    "        self._brain_name = self._env.brain_names[0]\n",
    "        self._brain = self._env.brains[self._brain_name]\n",
    "\n",
    "        self._info = None\n",
    "        self._scores = None\n",
    "        self._score_history = ()\n",
    "\n",
    "    def generate_episode(self, agent, max_steps=None, train_mode=False):\n",
    "        \"\"\"Create a generator for and episode driven by an actor.\n",
    "        Args:\n",
    "            actor: An actor that provides the next action for a given state.\n",
    "            max_steps: Maximum number of steps (int) to take in the episode. If\n",
    "                None, the episode is generated until a terminal state is reached.\n",
    "\n",
    "        Returns:\n",
    "            A generator providing a tuple of the current state, the action taken,\n",
    "            the obtained reward, the next state and a flag whether the next\n",
    "            state is terminal or not.\n",
    "        \"\"\"\n",
    "        states = self.reset(train_mode=train_mode)\n",
    "        is_terminal = False\n",
    "        count = 0\n",
    "\n",
    "        while not is_terminal and (max_steps is None or count < max_steps):\n",
    "            actions = agent.act(states)\n",
    "            rewards, next_states, is_terminals = self.step(actions)\n",
    "\n",
    "            step_data = (states, actions, rewards, next_states, is_terminals)\n",
    "\n",
    "            states = next_states\n",
    "            is_terminal = np.any(is_terminals)\n",
    "            count += 1\n",
    "            if np.any(np.isnan(states)):\n",
    "                raise ValueError(\"Nan from env\")\n",
    "            # print(\"step\" + str(count))\n",
    "\n",
    "            yield step_data\n",
    "\n",
    "        self._score_history += (self.get_score(), )\n",
    "        self._scores = None\n",
    "\n",
    "    def reset(self, train_mode=False):\n",
    "        \"\"\"Reset and initiate a new episode in the environment.\n",
    "\n",
    "        Args:\n",
    "            train_mode: Indicate if the environment should be initiated in\n",
    "                training mode or not.\n",
    "\n",
    "        Returns:\n",
    "            The initial state of the episode (np.array).\n",
    "        \"\"\"\n",
    "        # if self._info is not None and not np.any(self._info.local_done):\n",
    "        #     raise Exception(\"Env is active, call terminate first\")\n",
    "\n",
    "        if self._scores is not None:\n",
    "            self._score_history += (np.mean(self._scores))\n",
    "\n",
    "        self._info = self._env.reset(train_mode=train_mode)[self._brain_name]\n",
    "        self._scores = np.zeros(self.get_agent_size())\n",
    "\n",
    "        return self._info.vector_observations\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"Execute an action on all instances.\n",
    "\n",
    "        Args:\n",
    "            action: An tensor of ints representing the actions each instance.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the rewards (floats), the next states (np.array) and\n",
    "            a booleans indicating if the next state is terminal or not.\n",
    "        \"\"\"\n",
    "        if self._info is None:\n",
    "            raise Exception(\"Env is not active, call reset first\")\n",
    "\n",
    "        if torch.is_tensor(actions):\n",
    "            actions = actions.numpy()\n",
    "\n",
    "        self._info = self._env.step(actions)[self._brain_name]\n",
    "        next_states = self._info.vector_observations\n",
    "        rewards = self._info.rewards\n",
    "        is_terminals = self._info.local_done\n",
    "        self._scores += np.array(rewards)\n",
    "\n",
    "        return rewards, next_states, is_terminals\n",
    "\n",
    "    def terminate(self):\n",
    "        self._info = None\n",
    "        if self._scores is not None:\n",
    "            self._score_history += (self.get_score(), )\n",
    "        self._scores = None\n",
    "\n",
    "    def close(self):\n",
    "        self.terminate()\n",
    "        self._env.close()\n",
    "\n",
    "    def get_score(self):\n",
    "        \"\"\"Return the cumulative average reward of the current episode.\"\"\"\n",
    "        return np.mean(self._scores) if self._scores is not None else self._score_history[-1]\n",
    "\n",
    "    def get_score_history(self):\n",
    "        \"\"\"Return the cumulative average reward of all episodes.\"\"\"\n",
    "        return self._score_history\n",
    "\n",
    "    def clear_score_history(self):\n",
    "        \"\"\"Clear the cumulative average reward of all episodes.\"\"\"\n",
    "        self._scores = None\n",
    "        self._score_history = ()\n",
    "\n",
    "    def get_agent_size(self):\n",
    "        if self._info is None:\n",
    "            raise ValueError(\"No agents are initialized\")\n",
    "\n",
    "        return len(self._info.agents)\n",
    "\n",
    "    def get_action_size(self):\n",
    "        return self._brain.vector_action_space_size\n",
    "\n",
    "    def get_state_size(self):\n",
    "        return self._brain.vector_observation_space_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate episodes on the environment the *CoControlAgent* class provides an agent that selects actions in a particular state of the environement based on a policy function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s CoControlAgent cocontrol/environment\n",
    "class CoControlAgent:\n",
    "    \"\"\"Agent based on a policy approximator.\"\"\"\n",
    "\n",
    "    def __init__(self, policy):\n",
    "        \"\"\"Initialize the agent.\n",
    "\n",
    "        Args:\n",
    "            pi: policy-function that is callable with n states and returns a\n",
    "                (n, a)-dim array-like containing the value of each action.\n",
    "        \"\"\"\n",
    "        self._policy = policy\n",
    "\n",
    "    def act(self, states):\n",
    "        \"\"\"Select actions for the given states.\n",
    "\n",
    "        Args:\n",
    "            state: An array-like of states to choose the actions for.\n",
    "        Returns:\n",
    "            An array-like of floats representing the actions.\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(states):\n",
    "            try:\n",
    "                states = torch.from_numpy(states, dtype=torch.float)\n",
    "            except:\n",
    "                states = torch.from_numpy(np.array(states, dtype=np.float))\n",
    "        else:\n",
    "            states = states.float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return self._policy.sample(states)\n",
    "\n",
    "    def get_policy(self):\n",
    "        return self._policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can setup and test the environment and run an episode with a random or dummy policy. To run the test, uncomment the *test_env()* invocation and choose the policy you like.\n",
    "\n",
    "**_After you ran the test you need to comment it out again and restart the kernel, as recreating the environment does not work!_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded from Reacher.app\n"
     ]
    }
   ],
   "source": [
    "env = CoControlEnv()\n",
    "\n",
    "def test_env():\n",
    "    # all actions are between -1 and 1\n",
    "    class DummyPolicy:\n",
    "        def sample(self, state):\n",
    "            return torch.rand(env.get_agent_size(), env.get_action_size()) * 2.0 - 1.0\n",
    "\n",
    "    agent = CoControlAgent(DummyPolicy())\n",
    "    episode = enumerate(env.generate_episode(agent, max_steps = 1000))\n",
    "    for count, step_data in episode:\n",
    "        # Consume the generated steps\n",
    "        pass\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    \n",
    "##### UNCOMMENT THIS TO TEST, COMMENT OUT AND RESTART THE KERNEL AFTERWARDS ####\n",
    "\n",
    "# test_env()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "Size of each states: 33\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "states = env.reset()\n",
    "\n",
    "# number of agents\n",
    "num_agents = env.get_agent_size()\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = env.get_action_size()\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state_size = env.get_state_size()\n",
    "print('Size of each states:', state_size)\n",
    "\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], states.shape[1]))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "env.clear_score_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Models and policy\n",
    "\n",
    "The models to appoximate the action means of the policy distributions (*Actor*) and the state-value function (*Critic*) are implemented as PyTorch *nn.Module*. As they share the same hidden layers, they extend a common base class, and only add their individual output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s PPOModel cocontrol/model\n",
    "class PPOModel(nn.Module):\n",
    "    \"\"\"Base module for Actor and Critic models.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, output_size, seed, fc1_units, fc2_units):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            output (int): Dimension of the output\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(PPOModel, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, output_size)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*self._init_limits(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*self._init_limits(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def _init_limits(self, layer):\n",
    "        input_size = layer.weight.data.size()[0]\n",
    "        lim = 1. / np.sqrt(input_size)\n",
    "\n",
    "        return (-lim, lim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s Actor cocontrol/model\n",
    "class Actor(PPOModel):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=2, fc1_units=128, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__(state_size, action_size, seed, fc1_units, fc2_units)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> action means.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return F.tanh(self.fc3(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s Critic cocontrol/model\n",
    "class Critic(PPOModel):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, seed=2, fc1_units=128, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__(state_size, 1, seed, fc1_units, fc2_units)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an critic (value function) network that maps states -> values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy itself is implemented in a separate class that wraps the mean approximator and  provides normal distributions for each action. The *Policy* class can be used to calculate the log probabilities of given state-action pairs as well a to sample actions in a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s Policy cocontrol/model\n",
    "class Policy():\n",
    "    \"\"\"Policy that provides action probabilities and samples actions accordingly.\"\"\"\n",
    "\n",
    "    def __init__(self, model, sigma=1.0, cap=[-1.0, 1.0]):\n",
    "        \"\"\"Initialize parameters.\n",
    "        Params\n",
    "        ======\n",
    "            model fn: state -> means: A model that maps states to means of the\n",
    "                  action distributions\n",
    "            sigma (float): variance of the action distributions\n",
    "            cap (list): limits for the action values\n",
    "        \"\"\"\n",
    "        if sigma <= 0.0:\n",
    "            raise ValueError(\"sigma must be positive: \" + str(sigma))\n",
    "\n",
    "        self._model = model\n",
    "        self._sigma = sigma\n",
    "        self._cap_min = cap[0] if cap is not None else None\n",
    "        self._cap_max = cap[1] if cap is not None else None\n",
    "\n",
    "    def log_prob(self, states, actions):\n",
    "        \"\"\"Log-probabilities of the given actions for the given states.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        states (tensor): dimensions (agents, steps, state_size)\n",
    "        actions (tensor): dimensions (agents, steps, action_size)\n",
    "\n",
    "        Returns\n",
    "        ======\n",
    "        log probabilities (tensor): dimensions (agents, steps, 1)\n",
    "        \"\"\"\n",
    "        if not states.dim() == actions.dim() == 3:\n",
    "            raise ValueError(\"dimensions are too small, unsqueeze: \" + str(states.size()) + \"-\" + str(actions.size()))\n",
    "\n",
    "        if not states.size()[:2] == actions.size()[:2]:\n",
    "            raise ValueError(\"dimenions don't match: \" + str(states.size()) + \"-\" + str(actions.size()))\n",
    "\n",
    "        states = states.to(device, dtype=torch.float)\n",
    "        actions = actions.to(device, dtype=torch.float)\n",
    "\n",
    "        distributions = self.distributions(states)\n",
    "\n",
    "        log_probs = distributions.log_prob(actions).float()\n",
    "        if self._cap_max is not None:\n",
    "            boundary = torch.ge(actions, self._cap_max)\n",
    "            if boundary.any():\n",
    "                log_cdf = torch.log(1 - distributions.cdf(actions)).float()\n",
    "                log_probs = torch.where(boundary, log_cdf, log_probs)\n",
    "        if self._cap_min is not None:\n",
    "            boundary = torch.le(actions, self._cap_min)\n",
    "            if boundary.any():\n",
    "                log_cdf = torch.log(distributions.cdf(actions)).float()\n",
    "                log_probs = torch.where(boundary, log_cdf, log_probs)\n",
    "\n",
    "        return torch.sum(log_probs, dim=2).unsqueeze(2)\n",
    "\n",
    "    def distributions(self, states):\n",
    "        means = self._model(states.to(device, dtype=torch.float))\n",
    "\n",
    "        return dist.Normal(means, self._sigma)\n",
    "\n",
    "    def sample(self, states):\n",
    "        \"\"\"Sample actions for the given states according to the policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        states (tensor): dimensions (agents, steps, state_size)\n",
    "\n",
    "        Returns\n",
    "        ======\n",
    "        actions (tensor): dimensions (agents, steps, action_size)\n",
    "        \"\"\"\n",
    "        sample = self.distributions(states).sample()\n",
    "\n",
    "        if self._cap_min is None or self._cap_max is None:\n",
    "            return sample\n",
    "        else:\n",
    "            return torch.clamp(sample, self._cap_min, self._cap_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.4 PPO implementation\n",
    "\n",
    "The implementation of the algorithm is contained in the *PPOLearner* class that executes the the PPO algorithm with the given paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s PPOLearner cocontrol/training\n",
    "class PPOLearner():\n",
    "    \"\"\"Implementation of the PPO algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env=None,\n",
    "            episodes_in_epoch=8, ppo_epochs=8, batch_size=32,\n",
    "            window_size=50, window_step=2,\n",
    "            ppo_clip=0.75, sigma=0.5,\n",
    "            gamma=1.0, gae_tau=0.75, lr=1e-4):\n",
    "        # Don't instantiate as default as the constructor already starts the unity environment\n",
    "        self._env = env if env is not None else CoControlEnv()\n",
    "\n",
    "        self._state_size = self._env.get_state_size()\n",
    "        self._actions = self._env.get_action_size()\n",
    "\n",
    "        self._episodes_in_epoch = episodes_in_epoch\n",
    "        self._ppo_epochs = ppo_epochs\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        self._window_size = window_size\n",
    "        self._window_step = window_step\n",
    "\n",
    "        self._sigma = sigma\n",
    "        self._gamma = gamma\n",
    "        self._gae_tau = gae_tau\n",
    "\n",
    "        self._ppo_clip = ppo_clip\n",
    "        self._lr = lr\n",
    "\n",
    "        self._policy_model = Actor(self._state_size, self._actions).to(device)\n",
    "        self._value_model = Critic(self._state_size).to(device)\n",
    "\n",
    "        self._policy_optimizer = optim.Adam(self._policy_model.parameters(), lr, eps=1e-5)\n",
    "        self._value_optimizer = optim.Adam(self._value_model.parameters(), lr, eps=1e-5)\n",
    "\n",
    "        self._policy_model.eval()\n",
    "        self._value_model.eval()\n",
    "\n",
    "        print(\"Initialize PPOLearner with model:\")\n",
    "        print(self._policy_model)\n",
    "        print(self._value_model)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Store the learning result.\n",
    "\n",
    "        Store the parameters of the current models to the given path.\n",
    "        \"\"\"\n",
    "        torch.save(self._policy_model.state_dict(), \"actor_\" + path)\n",
    "        torch.save(self._value_model.state_dict(), \"critic_\" + path)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load learning results.\n",
    "\n",
    "        Load the parameters from the given path into the models.\n",
    "        \"\"\"\n",
    "        self._policy_model.load_state_dict(torch.load(\"actor_\" + path))\n",
    "        self._value_model.load_state_dict(torch.load(\"critic_\" + path))\n",
    "        self._policy_model.to(device)\n",
    "\n",
    "    def get_agent(self, sigma):\n",
    "        \"\"\"Return an agent based on the current policy model with the given variance.\n",
    "        \"\"\"\n",
    "        return CoControlAgent(self.get_policy(sigma))\n",
    "\n",
    "    def get_policy(self, sigma):\n",
    "        \"\"\"Return a policy based on the model of the learner.\n",
    "        \"\"\"\n",
    "        return Policy(self._policy_model, sigma)\n",
    "\n",
    "    def train(self, num_epochs=100):\n",
    "        for epoch in range(num_epochs):\n",
    "            policy = self.get_policy(self._sigma)\n",
    "\n",
    "            episodes = ( self._generate_episode(policy, epoch) for i in range(self._episodes_in_epoch) )\n",
    "            episodes = ( e for e in zip(*episodes) )\n",
    "\n",
    "            states, actions, rewards, next_states, is_terminals = \\\n",
    "                    [ torch.cat(component, dim=0).detach() for component in episodes ]\n",
    "\n",
    "            returns = self._calculate_returns(rewards).detach()\n",
    "            log_probs = policy.log_prob(states, actions).detach()\n",
    "            values = self._value_model(states).detach()\n",
    "            next_values = self._value_model(next_states).detach()\n",
    "\n",
    "            td_errors = rewards + self._gamma * next_values - values\n",
    "            advantages = self._calculate_advantages(td_errors)\n",
    "\n",
    "            advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "            print(\"Collected windows: \" + str(states.size()))\n",
    "\n",
    "            # Validate dimensions\n",
    "            for data in (states, actions, next_states, rewards, is_terminals,\n",
    "                    returns, advantages, values, log_probs):\n",
    "                assert len(set([ d.size()[0] for d in data ])) == 1\n",
    "            for data in (states, actions, next_states, rewards, is_terminals,\n",
    "                    returns, advantages, values, log_probs):\n",
    "                assert len(set([ d.size()[1] for d in data ])) == 1\n",
    "            assert self._env.get_state_size() == states.size()[2] == next_states.size()[2]\n",
    "            assert self._env.get_action_size() == actions.size()[2]\n",
    "            for data in (rewards, is_terminals, returns, advantages, values, log_probs):\n",
    "                assert 1 == data.size()[2]\n",
    "\n",
    "            self._value_model.train()\n",
    "            self._policy_model.train()\n",
    "\n",
    "            for ppo_epoch in range(self._ppo_epochs):\n",
    "                sampler = [idx for idx in self._random_sample(np.arange(states.size(0)), self._batch_size) ]\n",
    "                for batch_indices in sampler:\n",
    "                    batch_indices = torch.tensor(batch_indices).long()\n",
    "                    sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages = [\n",
    "                            data[batch_indices]\n",
    "                            for data in [states, actions, log_probs, returns, advantages] ]\n",
    "\n",
    "                    new_log_probs = policy.log_prob(sampled_states.detach(), sampled_actions.detach())\n",
    "                    ratio = (new_log_probs - sampled_log_probs_old.detach()).exp()\n",
    "\n",
    "                    obj = ratio * sampled_advantages.detach()\n",
    "                    obj_clipped = ratio.clamp(1.0 - self._ppo_clip, 1.0 + self._ppo_clip) \\\n",
    "                            * sampled_advantages.detach()\n",
    "                    policy_loss = - torch.min(obj, obj_clipped).mean()\n",
    "\n",
    "                    new_value = self._value_model(sampled_states.detach())\n",
    "                    value_loss = 0.5 * (sampled_returns.detach() - new_value).pow(2).mean()\n",
    "\n",
    "                    self._value_optimizer.zero_grad()\n",
    "                    value_loss.backward()\n",
    "                    self._value_optimizer.step()\n",
    "\n",
    "                    self._policy_optimizer.zero_grad()\n",
    "                    policy_loss.backward()\n",
    "                    self._policy_optimizer.step()\n",
    "\n",
    "                self._value_model.eval()\n",
    "                self._policy_model.eval()\n",
    "\n",
    "                yield policy_loss, self._env.get_score(), ppo_epoch == self._ppo_epochs - 1\n",
    "\n",
    "    def _generate_episode(self, policy, epoch):\n",
    "        agent = CoControlAgent(policy)\n",
    "\n",
    "        episode = ( step_data for step_data in self._env.generate_episode(agent, max_steps=1000, train_mode=True) )\n",
    "        episode = ( episode_data for episode_data in zip(*episode) )\n",
    "\n",
    "        # state = tuple of (1,33) arrays, etc.., concat along first dimension\n",
    "        states, actions, rewards, next_states, is_terminals = [\n",
    "                torch.stack(tuple(self._to_tensor(step)[0] for step in data), dim=1)\n",
    "                for data in episode ]\n",
    "\n",
    "        assert self._env.get_agent_size() == states.size()[0]\n",
    "\n",
    "        #split episodes\n",
    "        states, actions, rewards, next_states, is_terminals = [\n",
    "                self._split(data, self._window_size)\n",
    "                for data in [states, actions, rewards, next_states, is_terminals] ]\n",
    "\n",
    "        positive_rewards = torch.sum(rewards, dim=1).squeeze() > 0.0\n",
    "        states, actions, rewards, next_states, is_terminals = [\n",
    "                data[positive_rewards,:,:]\n",
    "                for data in [states, actions, rewards, next_states, is_terminals] ]\n",
    "\n",
    "        # Verify dimensions\n",
    "        assert states.size()[0] == actions.size()[0] == rewards.size()[0] \\\n",
    "                == next_states.size()[0] == is_terminals.size()[0]\n",
    "        assert self._env.get_state_size() == states.size()[2] == next_states.size()[2]\n",
    "        assert self._env.get_action_size() == actions.size()[2]\n",
    "        assert 1 == rewards.size()[2] == is_terminals.size()[2]\n",
    "\n",
    "        print(\"Generated episode: \" + str(states.size()[0]) + \"/\" + str(len(positive_rewards)) \\\n",
    "                + \" windows (\" + str(states.size()[1]) + \")\")\n",
    "\n",
    "        return (states, actions, rewards, next_states, is_terminals) if states.nelement() > 0 \\\n",
    "                else self._generate_episode(policy)\n",
    "\n",
    "    def _cat_component(self, episodes, component):\n",
    "        return torch.cat(episodes[component], dim=0)\n",
    "\n",
    "    def _split(self, x, split_size):\n",
    "        if x.size()[1] % split_size != 0:\n",
    "            raise ValueError(\"Illegal state, episode cannot be split: \" + str(x.size()))\n",
    "\n",
    "        splits = x.size()[1] // split_size\n",
    "        step = self._window_step\n",
    "\n",
    "        windows = x.view(splits*x.size()[0], split_size, x.size()[2])\n",
    "        shifted_windows = tuple([ x[:,i:-split_size+i,:].contiguous()\n",
    "                .view((splits-1)*x.size()[0], split_size, x.size()[2])\n",
    "                for i in range(step, split_size, step) ])\n",
    "\n",
    "        return torch.cat((windows,) + shifted_windows, dim=0)\n",
    "\n",
    "    def _to_tensor(self, *arrays, dtype=torch.float):\n",
    "        results = [ torch.tensor(a).to(device, dtype=dtype) if not torch.is_tensor(a) else a\n",
    "                for a in arrays ]\n",
    "\n",
    "        return tuple(result.unsqueeze(dim=1) if result.dim() == 1 else result\n",
    "                for result in results)\n",
    "\n",
    "    def _random_sample(self, indices, batch_size):\n",
    "        indices = np.asarray(np.random.permutation(indices))\n",
    "        batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "        for batch in batches:\n",
    "            yield batch\n",
    "        r = len(indices) % batch_size\n",
    "        if r:\n",
    "            yield indices[-r:]\n",
    "\n",
    "    def _calculate_returns(self, rewards):\n",
    "        flipped = torch.flip(rewards, dims=(1,))\n",
    "        result = torch.zeros_like(flipped)\n",
    "        result[:,0,:] = flipped[:, 0, :]\n",
    "        for i in range(1, flipped.size()[1]):\n",
    "            result[:,i,:] = self._gamma * result[:,i-1,:] + flipped[:,i,:]\n",
    "\n",
    "        return torch.flip(result, dims=(1,))\n",
    "\n",
    "    def _calculate_advantages(self, td_errors):\n",
    "        flipped = torch.flip(td_errors, dims=(1,))\n",
    "        result = torch.zeros_like(flipped)\n",
    "        result[:,0,:] = flipped[:, 0, :]\n",
    "        for i in range(1, flipped.size()[1]):\n",
    "            result[:,i,:] = self._gamma * self._gae_tau * result[:,i-1,:] + flipped[:,i,:]\n",
    "\n",
    "        return torch.flip(result, dims=(1,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Training\n",
    "Now we can train the agent. For demonstration purposes we train the agent only for 10 epochs, and use a window step size of 5 to speed up training. This is, however, enough to get close to the optimal policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize PPOLearner with model:\n",
      "Actor(\n",
      "  (fc1): Linear(in_features=33, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=33, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tkb/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated episode: 248/3820 windows (50)\n",
      "Generated episode: 130/3820 windows (50)\n",
      "Generated episode: 263/3820 windows (50)\n",
      "Generated episode: 158/3820 windows (50)\n",
      "Generated episode: 269/3820 windows (50)\n",
      "Generated episode: 199/3820 windows (50)\n",
      "Generated episode: 203/3820 windows (50)\n",
      "Generated episode: 177/3820 windows (50)\n",
      "Collected windows: torch.Size([1647, 50, 33])\n",
      "\n",
      "Epoch: 1 - score history (all epochs): avg:+0.249 (min:+0.2/current:+0.2/max:+0.4)\n",
      "\n",
      "Generated episode: 92/3820 windows (50)\n",
      "Generated episode: 88/3820 windows (50)\n",
      "Generated episode: 106/3820 windows (50)\n",
      "Generated episode: 59/3820 windows (50)\n",
      "Generated episode: 55/3820 windows (50)\n",
      "Generated episode: 89/3820 windows (50)\n",
      "Generated episode: 68/3820 windows (50)\n",
      "Generated episode: 153/3820 windows (50)\n",
      "Collected windows: torch.Size([710, 50, 33])\n",
      "\n",
      "Epoch: 2 - score history (all epochs): avg:+0.180 (min:+0.1/current:+0.1/max:+0.4)\n",
      "\n",
      "Generated episode: 807/3820 windows (50)\n",
      "Generated episode: 771/3820 windows (50)\n",
      "Generated episode: 750/3820 windows (50)\n",
      "Generated episode: 903/3820 windows (50)\n",
      "Generated episode: 762/3820 windows (50)\n",
      "Generated episode: 743/3820 windows (50)\n",
      "Generated episode: 698/3820 windows (50)\n",
      "Generated episode: 825/3820 windows (50)\n",
      "Collected windows: torch.Size([6259, 50, 33])\n",
      "\n",
      "Epoch: 3 - score history (all epochs): avg:+0.461 (min:+0.1/current:+1.1/max:+1.2)\n",
      "\n",
      "Generated episode: 1527/3820 windows (50)\n",
      "Generated episode: 1417/3820 windows (50)\n",
      "Generated episode: 1488/3820 windows (50)\n",
      "Generated episode: 1323/3820 windows (50)\n",
      "Generated episode: 1498/3820 windows (50)\n",
      "Generated episode: 1459/3820 windows (50)\n",
      "Generated episode: 1597/3820 windows (50)\n",
      "Generated episode: 1273/3820 windows (50)\n",
      "Collected windows: torch.Size([11582, 50, 33])\n",
      "\n",
      "Epoch: 4 - score history (all epochs): avg:+0.862 (min:+0.1/current:+1.7/max:+2.3)\n",
      "\n",
      "Generated episode: 2123/3820 windows (50)\n",
      "Generated episode: 2068/3820 windows (50)\n",
      "Generated episode: 1983/3820 windows (50)\n",
      "Generated episode: 2023/3820 windows (50)\n",
      "Generated episode: 2139/3820 windows (50)\n",
      "Generated episode: 2215/3820 windows (50)\n",
      "Generated episode: 2212/3820 windows (50)\n",
      "Generated episode: 1960/3820 windows (50)\n",
      "Collected windows: torch.Size([16723, 50, 33])\n",
      "\n",
      "Epoch: 5 - score history (all epochs): avg:+1.488 (min:+0.1/current:+3.3/max:+4.3)\n",
      "\n",
      "Generated episode: 2354/3820 windows (50)\n",
      "Generated episode: 2435/3820 windows (50)\n",
      "Generated episode: 2930/3820 windows (50)\n",
      "Generated episode: 2501/3820 windows (50)\n",
      "Generated episode: 2431/3820 windows (50)\n",
      "Generated episode: 2594/3820 windows (50)\n",
      "Generated episode: 2699/3820 windows (50)\n",
      "Generated episode: 2624/3820 windows (50)\n",
      "Collected windows: torch.Size([20568, 50, 33])\n",
      "\n",
      "Epoch: 6 - score history (all epochs): avg:+2.470 (min:+0.1/current:+7.9/max:+9.0)\n",
      "\n",
      "Generated episode: 3266/3820 windows (50)\n",
      "Generated episode: 3418/3820 windows (50)\n",
      "Generated episode: 3493/3820 windows (50)\n",
      "Generated episode: 3441/3820 windows (50)\n",
      "Generated episode: 3248/3820 windows (50)\n",
      "Generated episode: 3431/3820 windows (50)\n",
      "Generated episode: 3316/3820 windows (50)\n",
      "Generated episode: 3166/3820 windows (50)\n",
      "Collected windows: torch.Size([26779, 50, 33])\n",
      "\n",
      "Epoch: 7 - score history (all epochs): avg:+4.238 (min:+0.1/current:+13.7/max:+15.8)\n",
      "\n",
      "Generated episode: 3804/3820 windows (50)\n",
      "Generated episode: 3802/3820 windows (50)\n",
      "Generated episode: 3800/3820 windows (50)\n",
      "Generated episode: 3809/3820 windows (50)\n",
      "Generated episode: 3790/3820 windows (50)\n",
      "Generated episode: 3796/3820 windows (50)\n",
      "Generated episode: 3807/3820 windows (50)\n",
      "Generated episode: 3801/3820 windows (50)\n",
      "Collected windows: torch.Size([30409, 50, 33])\n",
      "\n",
      "Epoch: 8 - score history (all epochs): avg:+7.296 (min:+0.1/current:+28.1/max:+29.9)\n",
      "\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3819/3820 windows (50)\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3819/3820 windows (50)\n",
      "Collected windows: torch.Size([30558, 50, 33])\n",
      "\n",
      "Epoch: 9 - score history (all epochs): avg:+10.655 (min:+0.1/current:+38.2/max:+38.2)\n",
      "\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Generated episode: 3820/3820 windows (50)\n",
      "Collected windows: torch.Size([30560, 50, 33])\n",
      "\n",
      "Epoch: 10 - score history (all epochs): avg:+13.495 (min:+0.1/current:+39.1/max:+39.1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tkb/anaconda3/envs/drlnd/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/tkb/anaconda3/envs/drlnd/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "episode_cnt = 0\n",
    "for cnt, data in enumerate(PPOLearner(env=env, window_step=5).train(10)):\n",
    "    performance, score, terminal = data\n",
    "\n",
    "    if terminal:\n",
    "        episode_cnt += 1\n",
    "        print(\"\\nEpoch: {} - score history (all epochs): avg:{:+.3f} (min:{:+.1f}/current:{:+.1f}/max:{:+.1f})\\n\".format(\n",
    "                episode_cnt, np.mean(env.get_score_history()),\n",
    "                min(env.get_score_history()), env.get_score(), max(env.get_score_history())))\n",
    "\n",
    "        \n",
    "plot(env.get_score_history(), windows=[1], colors=['b'], labels=[\"Score\"], path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Running the trained agent\n",
    "Finally let's watch the trained agents doing their work! We load the stored parameters files and execute the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize PPOLearner with model:\n",
      "Actor(\n",
      "  (fc1): Linear(in_features=33, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=33, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Score:    39.15849912473932\n"
     ]
    }
   ],
   "source": [
    "learner = PPOLearner(env=env)\n",
    "learner.load(\"parameters.pt\")\n",
    "replay_agent = learner.get_agent(0.2)\n",
    "\n",
    "episode = env.generate_episode(replay_agent)\n",
    "\n",
    "for count, step_data in enumerate(episode):\n",
    "    # Consume the generated steps\n",
    "    pass\n",
    "\n",
    "print(\"Score:    \" + str(env.get_score()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
